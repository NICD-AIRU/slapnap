---
title: 'SLAPNAP Report: `r format(strsplit(Sys.getenv("Nab"), split = ";")[[1]])`'
author: "SLAPNAP Team"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
source("/home/lib/report_preamble.R")
```

```{r load_cvfit_objects}
if (!no_cv) {
    cvfit_ic50 <- readRDS("/home/slfits/cvfit_log10.pc.ic50.rds")
    class(cvfit_ic50) <- "myCV.SuperLearner"
    if(!reduce_outcomes){
    	cvfit_ic80 <- readRDS("/home/slfits/cvfit_log10.pc.ic80.rds")
    	cvfit_iip <- readRDS("/home/slfits/cvfit_iip.rds")
    	cvfit_dichotomous.1 <- readRDS("/home/slfits/cvfit_dichotomous.1.rds")
    	cvfit_dichotomous.2 <- readRDS("/home/slfits/cvfit_dichotomous.2.rds")
    	class(cvfit_ic80) <- class(cvfit_iip) <- class(cvfit_dichotomous.1) <-
    		class(cvfit_dichotomous.2) <- "myCV.SuperLearner"
    }
} else {
    cvfit_ic50 <- readRDS("/home/slfits/fit_log10.pc.ic50.rds")
    class(cvfit_ic50) <- "myCV.SuperLearner"
    if (!reduce_outcomes) {
        cvfit_ic80 <- readRDS("/home/slfits/fit_log10.pc.ic80.rds")
    	cvfit_iip <- readRDS("/home/slfits/fit_iip.rds")
    	cvfit_dichotomous.1 <- readRDS("/home/slfits/fit_dichotomous.1.rds")
    	cvfit_dichotomous.2 <- readRDS("/home/slfits/fit_dichotomous.2.rds")
        class(cvfit_ic80) <- class(cvfit_iip) <- class(cvfit_dichotomous.1) <-
    		class(cvfit_dichotomous.2) <- "myCV.SuperLearner"
    }
}
rslt_ic50 <- get_est_and_ci(cvfit_ic50, Rsquared = TRUE)
if (!reduce_outcomes) {
    rslt_ic80 <- get_est_and_ci(cvfit_ic80, Rsquared = TRUE)
    rslt_iip <- get_est_and_ci(cvfit_iip, Rsquared = TRUE)
    rslt_dichot1 <- get_est_and_ci(cvfit_dichotomous.1)
    rslt_dichot2 <- get_est_and_ci(cvfit_dichotomous.2)
} else {
    rslt_ic80 <- rslt_iip <- rslt_dichot1 <- rslt_dichot2 <- rep(NA, 3)
}
# # get summary measures for executive summary
# sum_ic50 <- summary(cvfit_ic50, Rsquared = TRUE, method = "method.CC_LS")
# sum_ic80 <- summary(cvfit_ic80, Rsquared = TRUE, method = "method.CC_LS")
# sum_iip <- summary(cvfit_iip, Rsquared = TRUE, method = "method.CC_LS")
# sum_dichot1 <- summary(cvfit_dichotomous.1, Rsquared = FALSE, method = "method.AUC")
# sum_dichot2 <- summary(cvfit_dichotomous.2, Rsquared = FALSE, method = "method.AUC")
```

# Executive summary

The NAbs studied in this analysis are `r antibodies`. There were `r n_row_prev` observations extracted from the CATNAP database. There were `r n_row_now` observations with complete sequence data that were used in the analysis.

```{r rsquaredtable}
if (no_cv) {
    print("No R-squared table available if cv is not used.")
} else {
    if (reduce_outcomes) {
        print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
    }
    rsqtab <- rbind(unlist(rslt_ic50), unlist(rslt_ic80), unlist(rslt_iip))
    row.names(rsqtab) <- c("IC-50","IC-80","IIP")
    kable(rsqtab, col.names = c(expression(CV-R^2), "Lower 95% CI", "Upper 95% CI"),
          digits = 3, row.names = TRUE,
          caption = "Estimates of cross-validated R-squared for super learner predictions of the three continuous-valued outcomes")
}
```

```{r auctable}
if (no_cv) {
    print("No AUC table available if cv is not used.")
} else {
    if (reduce_outcomes) {
        print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
    }
    auctab <- rbind(unlist(rslt_dichot1), unlist(rslt_dichot2))
    row.names(auctab) <- c("Estimated sensitivity", "Multiple sensitivity")
    kable(auctab, col.names = c("CV-AUC", "Lower 95% CI", "Upper 95% CI"),
          digits = 3, row.names = TRUE,
          caption = "Estimates of cross-validated AUC for super learner predictions of the two dichotomous outcomes")
}
```

The estimated cross-validated $R^2$ of the super learner for predicting continuous outcomes are shown in Table \@ref(tab:rsquaredtable). The estimated cross-validated area under the receiver operating characteristic curve (AUC) of the super learner for predicting binary sensitivity measures are shown in Table \@ref(tab:auctable).

The estimated variable importance of individual features in the Super Learner are shown in Table \@ref(tab:combinedvarimpindividualfeatures). These are features that were deemed important for prediction of at least two of the outcomes. For features associated with each outcome, please refer to the outcomes' respective sections of the report.

```{r combinedvarimpindividualfeatures}
kable(imp_overall, caption = "Variable importance of individual features in the Super Learner, defined by being label as 'important' for prediction of at least two of the outcomes considered; these outcomes are listed in the table.")
```

In Tables \@ref(tab:vimpgroupcond)--\@ref(tab:vimpindimarg), we display the groups of variables and individual variables with population variable importance statistically significantly greater than zero for each outcome. For the population variable importance of each group and individual feature for predicting each outcome, please refer to the outcomes' respective sections in Section \@ref(population-variable-importance-results).

```{r vimpgroupcond}
## vimp_summary_tbl created in preamble
if (no_cv) {
    print("No population variable importance table if CV is not used.")
} else {
    vimp_summary_cond <- vimp_summary_tbl %>%
        filter(grepl("Group", import_type) & grepl("conditional", import_type)) %>%
        mutate(tab_outcome_name = vimp_plot_name(outcome)) %>%
        select(est, cil, ciu, p_value, tab_outcome_name, group)
    kable(vimp_summary_cond, col.names = c("Estimated variable importance", "Lower 95% CI", "Upper 95% CI", "p-value", "Outcome name", "Variable group"), caption = "Conditional variable importance of groups relative to the remaining features that were deemed to have population importance statistically significantly different from zero for the specified outcome. Importance is measured via R^2 for continuous outcomes (IC-50, IC-80, IIP) and via AUC for binary outcomes (Estimated sensitivity, Multiple Sensitivity).")
}
```

```{r vimpgroupmarg}
## vimp_summary_tbl created in preamble
if (no_cv) {
    print("No population variable importance table if CV is not used.")
} else {
    vimp_summary_marg <- vimp_summary_tbl %>%
        filter(grepl("Group", import_type) & grepl("marginal", import_type)) %>%
        mutate(tab_outcome_name = vimp_plot_name(outcome)) %>%
        select(est, cil, ciu, p_value, tab_outcome_name, group)
    kable(vimp_summary_marg, col.names = c("Estimated variable importance", "Lower 95% CI", "Upper 95% CI", "p-value", "Outcome name", "Variable group"), caption = "Marginal variable importance of groups relative to the geographic confounders only that were deemed to have population importance statistically significantly different from zero for the specified outcome. Importance is measured via R^2 for continuous outcomes (IC-50, IC-80, IIP) and via AUC for binary outcomes (Estimated sensitivity, Multiple Sensitivity).")
}
```

```{r vimpgindimarg}
## vimp_summary_tbl created in preamble
if (no_cv) {
    print("No population variable importance table if CV is not used.")
} else {
    vimp_summary_indi <- vimp_summary_tbl %>%
        filter(grepl("Individual", import_type)) %>%
        mutate(tab_outcome_name = vimp_plot_name(outcome)) %>%
        select(est, cil, ciu, p_value, tab_outcome_name, group)
    kable(vimp_summary_indi, col.names = c("Estimated variable importance", "Lower 95% CI", "Upper 95% CI", "p-value", "Outcome name", "Variable group"), caption = "Marginal variable importance of individual features relative to the geographic confounders only that were deemed to have population importance statistically significantly different from zero for the specified outcome. Importance is measured via R^2 for continuous outcomes (IC-50, IC-80, IIP) and via AUC for binary outcomes (Estimated sensitivity, Multiple Sensitivity).")
}
```

The distribution of amino acid at the important residues are shown for sensitive and resistant viruses in Figures \@ref(fig:logoPlot1) and \@ref(fig:logoPlot2).

```{r logoPlot1, fig.cap = "Distribution of amino acids at important residues for estimated sensitive versus resistant viruses", fig.height = 12, fig.width = 10}
## plotting functions sourced in preamble
if(!is.na(imp_overall$Variable[1])){
  all_imp_vars <- imp_overall$Variable
  imp_aa_vars <- imp_overall$Variable[grepl("hxb", imp_overall$Variable)]
  aa_positions <- sort(as.numeric(stringr::word(imp_aa_vars, 2 , 2, sep = '\\.')))
  make_logo_plot(data = dat, outcome_name = "dichotomous.1", aa_positions = aa_positions)
}else{
  print("No features qualified as important for predicting multiple outcomes")
}
```

```{r logoPlot2, fig.cap = "Distribution of amino acids at important residues for multiply sensitive versus resistant viruses", fig.height = 12, fig.width = 10}
if(!is.na(imp_overall$Variable[1])){
  all_imp_vars <- imp_overall$Variable
  imp_aa_vars <- imp_overall$Variable[grepl("hxb", imp_overall$Variable)]
  aa_positions <- sort(as.numeric(stringr::word(imp_aa_vars, 2 , 2, sep = '\\.')))
  make_logo_plot(data = dat, outcome_name = "dichotomous.2", aa_positions = aa_positions)
}else{
  print("No features qualified as important for predicting multiple outcomes")
}
```

# Summary of endpoints

Histograms of IC-50 and IC-80 for each individual NAb are shown in Figure \@ref(fig:histic50) and Figure \@ref(fig:histic80), respectively. Related summaries are shown in Table \@ref(tab:tableic50) and Table \@ref(tab:tableic80).

Histograms for the predicted IC-50 an IC-80 based on the additive model are shown in Figure \@ref(fig:histcombn), a histogram of the predicted IIP is shown in Figure \@ref(fig:histiip), and related summaries are shown in Table \@ref(tab:combntable). The distribution of the two binary sensitivity measures is shown in Figure \@ref(fig:dichot).

```{r histic50, fig.cap = "Histogram of IC50 values for individual NAbs", fig.height=5}
ic50_hist <- list()
i50_summ_tab <- list()
ct <- 0
for(i in seq_len(n_ab)){
	ct <- ct + 1
	this_name <- gsub("-", ".", paste0(antibodies[i], ".ic50.imputed"))
	ic50_hist[[ct]] <- make_hist_plot(dat, var_name = this_name,
	                                  x_lab = paste0("IC-50 ", antibodies[i]),
	                                  y_lab = "Density")
	i50_summ_tab[[ct]] <- summary(dat[, this_name])[1:6] # to ignore NA columns
	ct <- ct+1
 	dat[,paste0("log10_",this_name)] <- log10(dat[, this_name])
 	ic50_hist[[ct]] <- make_hist_plot(dat, var_name = paste0("log10_",this_name),
 	                                  x_lab = bquote(log[10]~"(IC-50 "~.(antibodies[i])~")"),
 	                                  y_lab = "")
 	i50_summ_tab[[ct]] <- summary(dat[, paste0("log10_",this_name)])[1:6]
}
do.call(grid.arrange, ic50_hist)
```

```{r tableic50}
ic50_table <- Reduce(rbind, i50_summ_tab)
row.names(ic50_table) <- c(paste0("IC-50 ", antibodies),
                           paste0("log(IC-50 ", antibodies,")"))[c(seq(1,n_ab*2,by=2),seq(2, n_ab*2, by = 2))]
kable(ic50_table, row.names = TRUE, digits = 3,
      caption = "Summary statistics of IC-50 values for individual NAbs")
```

```{r histic80, fig.cap = "Histogram of IC80 values for individual NAbs"}
ic80_hist <- list()
i80_summ_tab <- list()
ct <- 0
for(i in seq_len(n_ab)){
	ct <- ct + 1
	this_name <- gsub("-", ".", paste0(antibodies[i], ".ic80.imputed"))
	ic80_hist[[ct]] <- make_hist_plot(dat, var_name = this_name,
	                                  x_lab = paste0("IC-80 ", antibodies[i]),
	                                  y_lab = "Density")
	i80_summ_tab[[ct]] <- summary(dat[, this_name])[1:6] # to ignore NA columns
	ct <- ct+1
 	dat[,paste0("log10_",this_name)] <- log10(dat[, this_name])
 	ic80_hist[[ct]] <- make_hist_plot(dat, var_name = paste0("log10_",this_name),
	                                  x_lab = bquote(log[10]~"(IC-80 "~.(antibodies[i])~")"),
	                                  y_lab = "")
 	i80_summ_tab[[ct]] <- summary(dat[, paste0("log10_",this_name)])[1:6]
}
do.call(grid.arrange, ic80_hist)
```

```{r tableic80}
ic80_table <- Reduce(rbind, i80_summ_tab)
row.names(ic80_table) <- c(paste0("IC-80 ", antibodies),
                           paste0("log(IC-80 ", antibodies,")"))[c(seq(1,n_ab*2,by=2),seq(2, n_ab*2, by = 2))]
kable(ic80_table, row.names = TRUE, digits = 3,
      caption = "Summary statistics of IC-80 values for individual NAbs")
```


```{r histcombn, fig.cap = "Histogram of estimated IC50 and IC80 for combined NAbs. Top row = original scale; bottom row = log10 scale"}
combn_hist <- list()
combn_hist[[1]] <- make_hist_plot(dat, "pc.ic50", "Estimated IC-50", "Density")
combn_hist[[2]] <- make_hist_plot(dat, "log10.pc.ic50", bquote(log[10]~"(estimated IC-50)"), "")
combn_hist[[3]] <- make_hist_plot(dat, "pc.ic80", "Estimated IC-80", "Density")
combn_hist[[4]] <- make_hist_plot(dat, "log10.pc.ic80", bquote(log[10]~"(estimated IC-80)"), "")
do.call(grid.arrange, combn_hist)
```

```{r histiip, fig.cap = "Estimated IIP for combined NAbs"}
iip_hist <- list()
iip_hist[[1]] <- make_hist_plot(dat, "iip", "Estimated IIP", "Density")
# dat$log10.iip <- log10(dat$iip)
# iip_hist[[2]] <- make_hist_plot(dat, "log10.iip", bquote(log[10]~"(estimated IIP)"), "")
do.call(grid.arrange, iip_hist)
```

```{r combntable}
# make a table of ic50 80 and iip for combined NAbs
combn_table <- rbind(
  summary(dat$pc.ic50)[1:6],
  summary(dat$log10.pc.ic50)[1:6],
  summary(dat$pc.ic80)[1:6],
  summary(dat$log10.pc.ic80)[1:6],
  summary(dat$iip)[1:6]
)
row.names(combn_table) <- c(
  "Estimated IC-50", "log(estimated IC-50)", "Estimated IC-80", "log(estimated IC-80)", "IIP"
)
kable(combn_table, row.names = TRUE, digits = 3,
      caption = "Summaries of combined NAb endpoints")
```

```{r dichot, fig.cap = "Distribution of binary sensitivity outcomes. Estimated indicates whether the estimated IC-50 falls above a sensitivity threshold. Multiple indicates whether at least two individual NAbs fall above a sensitivity threshold."}
n <- n_row_now
# make a stacked data set for easy use of geom_bar
dichot_dat <- data.frame(
  Outcome_type = c("Estimated", "Estimated", "Multiple", "Multiple"),
  Outcome = rep(c("Resistant", "Sensitive"), 2),
  value = 100*c(mean(dat$dichotomous.1), 1 - mean(dat$dichotomous.1),
            mean(dat$dichotomous.2), 1 - mean(dat$dichotomous.2))
)
trans_fill <- c(
  rgb(30, 21, 42, alpha = 255/2, maxColorValue = 255),
  rgb(78, 103, 102, alpha = 255/2, maxColorValue = 255),
  rgb(90,177,187, alpha = 255/2, maxColorValue = 255),
  rgb(165, 200, 130, alpha = 255/2, maxColorValue = 255),
  rgb(247, 221, 114, alpha = 255/2, maxColorValue = 255)
)

fill <- c(
  rgb(30, 21, 42, maxColorValue = 255),
  rgb(78, 103, 102, maxColorValue = 255),
  rgb(90,177,187, maxColorValue = 255),
  rgb(165, 200, 130, maxColorValue = 255),
  rgb(247, 221, 114, maxColorValue = 255)
)

ggplot(dichot_dat, aes(y = value, x = factor(Outcome_type), fill = factor(Outcome))) +
	geom_bar(position = "fill", stat = "identity") +
	scale_fill_manual(values = fill[c(2,4)]) +
	theme_bw() +
	labs(fill = "") +
	xlab("Sensitivity Measure") +
	geom_text(data = dichot_dat, aes(y = value, label = paste0(round(value, 2), "%"), stat = "identity"),
	          position = position_fill(vjust = 0.5))
```


# Super learner prediction results

```{r}
# get a fresh data set
dat <- get_dat()$dat
```

A super learner was fit to each outcome. For continuous outcomes, super learner weights were chosen to optimize estimated 10-fold cross-validated MSE; for binary outcomes, weights were chosen to optimize a 10-fold negative log-likelihood risk estimate. The algorithms used in the super learner are shown in Table \@ref(tab:superlearnerlibrarytable).

```{r superlearnerlibrarytable}
source("/home/lib/super_learner_libraries.R")
library_table <- data.frame()
this_library <- if(reduce_library){
	default_library_reduced
}else{
	default_library
}
Description <- sapply(this_library, function(x){
	eval(parse(text = paste0("descr_", x)))
})
knitr::kable(as.data.frame(Description),
             caption = "Super learner library")
```

The weights assigned to each algorithm for each outcome are shown in Table \@ref{tab:superlearnerweighttable}. Cross-validated risks for each outcome are shown in the following subsections.

```{r superlearnerweighttable}
# load super learner fits
outcome_names <- if(!reduce_outcomes){
	c("log10.pc.ic50", "log10.pc.ic80", "iip", "dichotomous.1", "dichotomous.2")
}else{
	c("log10.pc.ic50")
}
weight_list <- vector(mode = "list", length = length(outcome_names))
ct <- 0
for(o in outcome_names){
	ct <- ct + 1
	fit_name <- paste0("fit_", o, ".rds")
	weight_list[[ct]] <- readRDS(paste0("/home/slfits/slweights_", fit_name))
}
weight_table <- data.frame(Reduce(cbind, weight_list))
colnames(weight_table) <- outcome_names
row.names(weight_table) <- this_library
knitr::kable(as.data.frame(weight_table), digits = 2,
             caption = "Table of super learner weights for each outcome")
```

## IC-50

The cross-validated performance of super learner (in terms of $R^2$) relative to the discrete super learner and candidate algorithms are shown for IC-50 in Figure \@ref(fig:plotic50).

```{r plotic50, fig.cap = "Cross-validated R-squared for estimated IC-50"}
plot(cvfit_ic50, main_title = "IC-50", xlim1 = -0.05, xlim2 = 0.8, text_size = 9,
     method = "method.CC_LS")
# and get correlations
cor_ic50 <- get_cor_pred_outcome(cvfit_ic50)
```

Figure \@ref(fig:plotic50predversusoutcomes) shows cross-validated predictions made by the super learner plotted against outcomes. The Pearson correlation between predictions and true outcomes was `r round(cor_ic50[1], 3)`; the Spearman correlation was `r round(cor_ic50[2], 3)`.

```{r plotic50predversusoutcomes, fig.cap = "Cross-validated super learner predicted IC-50 plotted against actual IC-50 values. Colors correspond to different cross-validation folds"}
plot_cv_predictions(cvfit_ic50, outcome_name = "IC-50", log_axis = FALSE)
```

Figure \@ref(fig:plotic50imp) shows an animated representation of the importance of individual features for predicting estimated IC-50. To qualify as a top $x$ most important feature, the feature must satisfy two requirements: (1) it must have a Holm-adjusted p-value $<0.05$ in a model adjusting for geographic region; (2) it must be ranked as at least the $x$-th most important feature by one of the learning approaches (boosting, random forests, or LASSO). The animation visualizes the sequence in which features enter the set of important features by this definition.

```{r plotic50imp, fig.show='animate', animation.hook=my_webm, interval=0.2, fig.width=8, aniopts='controls', fig.cap='Animated importance of features for predicting IC50'}
for(m in 1:50){
	make_ml_import_plot(dat = dat, imp_df = imp_ic50, max_import = m)
}
```

Table \@ref(tab:tableic50imp) shows the top 15 most important features according to the two criteria described above. A feature is labeled "Sensitive" ("Resistant") if an increase in the value of that feature results in a lower (higher) value of the outcome.

```{r tableic50imp}
# ic50_tab is created in preamble
if (dim(ic50_tab)[1] != 0) {
    kable(ic50_tab, caption = "Top 10 most important features for predicting IC-50 and their importance ranks by each measure",
          col.names = c("Variable", "XGBOOST", "RF", "LASSO", "Adj. p-value", "Direction"),
          row.names = FALSE)
} else {
    print("No variables met the cutoff for importance.")
}
```

## IC-80

The cross-validated performance of super learner (in terms of $R^2$) relative to the discrete super learner and candidate algorithms are shown for IC-80 in Figure \@ref(fig:plotic80).

```{r plotic80, fig.cap = "Cross-validated R-squared for estimated IC-80"}
if (!reduce_outcomes) {
    plot(cvfit_ic80, main_title = "IC-80", xlim1 = -0.05, xlim2 = 1, text_size = 9,
         method = "method.CC_LS")
    # and get correlations
    cor_ic80 <- get_cor_pred_outcome(cvfit_ic80)
} else {
    cor_ic80 <- NA
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotic80predversusoutcomes) shows cross-validated predictions made by the super learner plotted against outcomes. The Pearson correlation between predictions and true outcomes was `r round(cor_ic80[1], 3)`; the Spearman correlation was `r round(cor_ic80[2], 3)`.

```{r plotic80predversusoutcomes, fig.cap = "Cross-validated super learner predicted IC-80 plotted against actual IC-80 values. Colors correspond to different cross-validation folds"}
if (!reduce_outcomes) {
    plot_cv_predictions(cvfit_ic80, outcome_name = "IC-80", log_axis = FALSE)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotic80imp) shows an animated representation of the importance of individual features for predicting estimate IC-80. To qualify as a top $x$ most important feature, the feature must satisfy two requirements: (1) it must have a Holm-adjusted p-value $<0.05$ in a model adjusting for geographic region; (2) it must be ranked as at least the $x$-th most important feature by one of the learning approaches (boosting, random forests, or LASSO). The animation visualizes the sequence in which features enter the set of important features by this definition.

```{r plotic80imp, fig.show='animate', animation.hook=my_webm, interval=0.2, fig.width=8, aniopts='controls', fig.cap='Animated importance of features for predicting IC-80'}
for(m in 1:50){
	make_ml_import_plot(dat = dat, imp_df = imp_ic80, max_import = m)
}
```

Table \@ref(tab:tableic80imp) shows the top 15 most important features according to the two criteria described above. A feature is labeled "Sensitive" ("Resistant") if an increase in the value of that feature results in a lower (higher) value of the outcome.

```{r tableic80imp}
# ic80_tab is created in preamble
if (dim(ic80_tab)[1] != 0) {
    kable(ic80_tab, caption = "Top 10 most important features for predicting IC-80 and their importance ranks by each measure",
          col.names = c("Variable", "XGBOOST", "RF", "LASSO", "Adj. p-value", "Direction"),
          row.names = FALSE)
} else {
    print("No variables met the cutoff for importance.")
}
```

## IIP

The cross-validated performance of super learner (in terms of $R^2$) relative to the discrete super learner and candidate algorithms are shown for IIP in Figure \@ref(fig:plotiip).

```{r plotiip, fig.cap = "Cross-validated R-squared for estimated IIP"}
if (!reduce_outcomes) {
    plot(cvfit_iip, main_title = "IIP", xlim1 = -0.05, xlim2 = 1, text_size = 9,
         method = "method.CC_LS")
    # and get correlations
    cor_iip <- get_cor_pred_outcome(cvfit_iip)
} else {
    cor_iip <- NA
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotiippredversusoutcomes) shows cross-validated predictions made by the super learner plotted against outcomes. The Pearson correlation between predictions and true outcomes was `r round(cor_iip[1], 3)`; the Spearman correlation was `r round(cor_iip[2], 3)`.

```{r plotiippredversusoutcomes, fig.cap = "Cross-validated super learner predicted IIP plotted against actual IIP values. Colors correspond to different cross-validation folds"}
if (!reduce_outcomes) {
    plot_cv_predictions(cvfit_iip, outcome_name = "IIP", log_axis = FALSE)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotiipimp) shows an animated representation of the importance of individual features for predicting estimate IC-80. To qualify as a top $x$ most important feature, the feature must satisfy two requirements: (1) it must have a Holm-adjusted p-value $<0.05$ in a model adjusting for geographic region; (2) it must be ranked as at least the $x$-th most important feature by one of the learning approaches (boosting, random forests, or LASSO). The animation visualizes the sequence in which features enter the set of important features by this definition.

```{r plotiipimp, fig.show='animate', animation.hook=my_webm, interval=0.2, fig.width=8, aniopts='controls', fig.cap='Animated importance of features for predicting IIP'}
for(m in 1:50){
	make_ml_import_plot(dat = dat, imp_df = imp_iip, max_import = m)
}
```

Table \@ref(tab:tableiipimp) shows the top 15 most important features according to the two criteria described above. A feature is labeled "Sensitive" ("Resistant") if an increase in the value of that feature results in a lower (higher) value of the outcome.

```{r tableiipimp}
# iip_tab is created in preamble
if (dim(iip_tab)[1] != 0) {
    kable(iip_tab, caption = "Top 10 most important features for predicting IIP and their importance ranks by each measure",
          col.names = c("Variable", "XGBOOST", "RF", "LASSO", "Adj. p-value", "Direction"),
          row.names = FALSE)
} else {
    print("No variables met the cutoff for importance.")
}
```


## Dichotomous sensitivity 1

The cross-validated performance of super learner (in terms of AUC) relative to the discrete super learner and candidate algorithms are shown for the first dichotomous endpoint (estimated IC-50 above sensitivity threshold) in Figure \@ref(fig:plotdichot1). Figure \@ref(fig:rocdichot1) shows cross-validated ROC curves for this endpoint.

```{r plotdichot1, fig.cap = "Cross-validated AUC for predicting sensitivity based on estimated IC-50"}
if (!reduce_outcomes) {
    plot(cvfit_dichotomous.1, main_title = "", xlim1 = 0.4, xlim2 = 1, text_size = 9,
         Rsquared = FALSE, method = "method.AUC")
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

```{r rocdichot1, fig.cap = "Cross-validated ROC curve for the super learner, discrete super learner, and single best performing algorithm for predicting sensitivity based on estimated IC-50"}
if (!reduce_outcomes) {
    plot_roc_curves(cvfit_dichotomous.1)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

```{r predprob1, fig.cap = "Predicted probabilities for resistant and sensitive viruses for the super learner, discrete super learner, and single best performing algorithm."}
if (!reduce_outcomes) {
    plot_predicted_prob_boxplots(cvfit_dichotomous.1, cols = trans_fill[c(2,4)])
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotdichot1imp) shows an animated representation of the importance of individual features for predicting estimated sensitivity. To qualify as a top $x$ most important feature, the feature must satisfy two requirements: (1) it must have a Holm-adjusted p-value $<0.05$ in a model adjusting for geographic region; (2) it must be ranked as at least the $x$-th most important feature by one of the learning approaches (boosting, random forests, or LASSO). The animation visualizes the sequence in which features enter the set of important features by this definition.

```{r plotdichot1imp, fig.show='animate', animation.hook=my_webm, interval=0.2, fig.width=8, aniopts='controls', fig.cap='Animated importance of features for predicting estimated sensitivity'}
for(m in 1:50){
	make_ml_import_plot(dat = dat, imp_df = imp_dichot1, max_import = m)
}
```

Table \@ref(tab:tabledichot1imp) shows the top 15 most important features according to the two criteria described above. A feature is labeled "Sensitive" ("Resistant") if an increase in the value of that feature results in a lower (higher) value of the outcome.

```{r tabledichot1imp}
# dichot1 is created in preamble
if (dim(ic80_tab)[1] != 0) {
    kable(dichot1_tab, caption = "Top 10 most important features for predicting estimated sensitivity and their importance ranks by each measure",
          col.names = c("Variable", "XGBOOST", "RF", "LASSO", "Adj. p-value", "Direction"),
          row.names = FALSE)
} else {
    print("No variables met the cutoff for importance.")
}
```


## Dichotomous sensitivity 2

The cross-validated performance of super learner (in terms of AUC) relative to the discrete super learner and candidate algorithms are shown for the second dichotomous endpoint (each individual IC-50 above threshold) in Figure \@ref(fig:plotdichot2). Figure \@ref(fig:rocdichot2) shows cross-validated ROC curves for this endpoint.

```{r plotdichot2, fig.cap = "Cross-validated AUC for predicting sensitivity based on sensitivity to individual NAbs"}
if (!reduce_outcomes) {
    plot(cvfit_dichotomous.2, main_title = "", xlim1 = 0.4, xlim2 = 1, text_size = 9,
         Rsquared = FALSE, method = "method.AUC")
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

```{r rocdichot2, fig.cap = "Cross-validated ROC curve for the super learner, discrete super learner, and single best performing algorithm for predicting sensitivity based on sensitivity to individual NAbs"}
if (!reduce_outcomes) {
    plot_roc_curves(cvfit_dichotomous.2)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

Figure \@ref(fig:plotdichot2imp) shows an animated representation of the importance of individual features for predicting multiple sensitivity. To qualify as a top $x$ most important feature, the feature must satisfy two requirements: (1) it must have a Holm-adjusted p-value $<0.05$ in a model adjusting for geographic region; (2) it must be ranked as at least the $x$-th most important feature by one of the learning approaches (boosting, random forests, or LASSO). The animation visualizes the sequence in which features enter the set of important features by this definition.

```{r plotdichot2imp, fig.show='animate', animation.hook=my_webm, interval=0.2, fig.width=8, aniopts='controls', fig.cap='Animated importance of features for predicting multiple sensitivity.'}
for(m in 1:50){
	make_ml_import_plot(dat = dat, imp_df = imp_dichot2, max_import = m)
}
```

Table \@ref(tab:tabledichot2imp) shows the top 15 most important features according to the two criteria described above. A feature is labeled "Sensitive" ("Resistant") if an increase in the value of that feature results in a lower (higher) value of the outcome.

```{r tabledichot2imp}
# dichot2_tab is created in preamble
if (dim(dichot2_tab)[1] != 0) {
    kable(dichot2_tab, caption = "Top 10 most important features for predicting multiple sensitivity and their importance ranks by each measure",
          col.names = c("Variable", "XGBOOST", "RF", "LASSO", "Adj. p-value", "Direction"),
          row.names = FALSE)
} else {
    print("No variables met the cutoff for importance.")
}
```

# Population variable importance results

We define the conditional population-level importance of a subgroup of features for predicting an outcome as the difference in population predictiveness between the best possible prediction function based on all available features versus all features except those under consideration. The marginal population-level importance of a subgroup of features is the difference in population predictiveness between the best possible prediction function based on the features under consideration plus geographic confounders versus only geographic confounders. We define predictiveness using R-squared for continuous outcomes (IC-50, IC-80, IIP) and using AUC for binary outcomes (Estimated sensitivity, Multiple sensitivity).

## IC-50

We show the population-level variable importance of groups of features in predicting IC-50 in Figure \@ref(fig:ic50groupvimp). The left-hand plot shows the conditional importance of the group relative to all other groups. The right-hand plot shows the marginal importance of the group relative to the null model with geographic confounders only.

```{r ic50groupvimp, fig.cap = "Group variable importance for predicting IC-50", fig.subcap = c("Conditional importance", "Marginal importance"), fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (no_cv) {
    ic50_grp_vimp_grob_lst <- log10.pc.ic50_vimp_plots
} else {
    ic50_grp_vimp_grob_lst <- log10.pc.ic50_cv_vimp_plots
}
grid.arrange(grobs = list(ic50_grp_vimp_grob_lst$conditional, ic50_grp_vimp_grob_lst$marginal), ncol = 2)
```
We show the individual-level variable importance for the top `r num_pop_import` variables in Figure \@ref(fig:ic50indivvimp). This importance is computed relative to the null model with geographic confounders only.

```{r ic50indivvimp, fig.cap = "Individual variable importance for predicting IC-50", fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (no_cv) {
    ic50_grp_vimp_grob_lst <- log10.pc.ic50_vimp_plots
} else {
    ic50_grp_vimp_grob_lst <- log10.pc.ic50_cv_vimp_plots
}
if (is.character(ic50_grp_vimp_grob_lst$individual)) {
    print(ic50_grp_vimp_grob_lst$individual)
} else {
    grid.arrange(grobs = list(ic50_grp_vimp_grob_lst$individual), ncol = 1)
}

```

## IC-80

We show the population-level variable importance of groups of features in predicting IC-80 in Figure \@ref(fig:ic80groupvimp). The left-hand plot shows the conditional importance of the group relative to all other groups. The right-hand plot shows the marginal importance of the group relative to the null model with geographic confounders only.

```{r ic80groupvimp, fig.cap = "Group variable importance for predicting IC-80", fig.subcap = c("Conditional importance", "Marginal importance"), fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        ic80_grp_vimp_grob_lst <- log10.pc.ic80_vimp_plots
    } else {
        ic80_grp_vimp_grob_lst <- log10.pc.ic80_cv_vimp_plots
    }
    grid.arrange(grobs = list(ic80_grp_vimp_grob_lst$conditional, ic80_grp_vimp_grob_lst$marginal), ncol = 2)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

We show the individual-level variable importance for the top `r num_pop_import` variables in Figure \@ref(fig:ic80indivvimp). This importance is computed relative to the null model with geographic confounders only.

```{r ic80indivvimp, fig.cap = "Individual variable importance for predicting IC-80", fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        ic80_grp_vimp_grob_lst <- log10.pc.ic80_vimp_plots
    } else {
        ic80_grp_vimp_grob_lst <- log10.pc.ic80_cv_vimp_plots
    }
    if (is.character(ic80_grp_vimp_grob_lst$individual)) {
        print(ic80_grp_vimp_grob_lst$individual)
    } else {
        grid.arrange(grobs = list(ic80_grp_vimp_grob_lst$individual), ncol = 1)
    }
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

## IIP

We show the population-level variable importance of groups of features in predicting IIP in Figure \@ref(fig:iipgroupvimp). The left-hand plot shows the conditional importance of the group relative to all other groups. The right-hand plot shows the marginal importance of the group relative to the null model with geographic confounders only.

```{r iipgroupvimp, fig.cap = "Group variable importance for predicting IIP", fig.cap = c("Conditional importance", "Marginal importance"), fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        iip_grp_vimp_grob_lst <- iip_vimp_plots
    } else {
        iip_grp_vimp_grob_lst <- iip_cv_vimp_plots
    }
    grid.arrange(grobs = list(iip_grp_vimp_grob_lst$conditional, iip_grp_vimp_grob_lst$marginal), ncol = 2)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

We show the individual-level variable importance for the top `r num_pop_import` variables in Figure \@ref(fig:iipindivvimp). This importance is computed relative to the null model with geographic confounders only.

```{r iipindivvimp, fig.cap = "Individual variable importance for predicting IIP", fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        iip_grp_vimp_grob_lst <- iip_vimp_plots
    } else {
        iip_grp_vimp_grob_lst <- iip_cv_vimp_plots
    }
    if (is.character(iip_grp_vimp_grob_lst$individual)) {
        print(iip_grp_vimp_grob_lst$individual)
    } else {
        grid.arrange(grobs = list(iip_grp_vimp_grob_lst$individual), ncol = 1)
    }
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

## Dichotomous sensitivity 1

We show the population-level variable importance of groups of features in predicting estimated sensitivity in Figure \@ref(fig:dichot1groupvimp). The left-hand plot shows the conditional importance of the group relative to all other groups. The right-hand plot shows the marginal importance of the group relative to the null model with geographic confounders only.

```{r dichot1groupvimp, fig.cap = "Group variable importance for predicting estimated sensitivity", fig.subcap = c("Conditional importance", "Marginal importance"), fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        dichotomous.1_grp_vimp_grob_lst <- dichotomous.1_vimp_plots
    } else {
        dichotomous.1_grp_vimp_grob_lst <- dichotomous.1_cv_vimp_plots
    }
    grid.arrange(grobs = list(dichotomous.1_grp_vimp_grob_lst$conditional, dichotomous.1_grp_vimp_grob_lst$marginal), ncol = 2)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

We show the individual-level variable importance for the top `r num_pop_import` variables in Figure \@ref(fig:dichot1indivvimp). This importance is computed relative to the null model with geographic confounders only.

```{r dichot1indivvimp, fig.cap = "Individual variable importance for predicting estimated sensitivity", fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        dichotomous.1_grp_vimp_grob_lst <- dichotomous.1_vimp_plots
    } else {
        dichotomous.1_grp_vimp_grob_lst <- dichotomous.1_cv_vimp_plots
    }
    if (is.character(dichotomous.1_grp_vimp_grob_lst$individual)) {
        print(dichotomous.1_grp_vimp_grob_lst$individual)
    } else {
        grid.arrange(grobs = list(dichotomous.1_grp_vimp_grob_lst$individual), ncol = 1)
    }
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

## Dichotomous sensitivity 2

We show the population-level variable importance of groups of features in predicting multiple sensitivity in Figure \@ref(fig:dichot2groupvimp). The left-hand plot shows the conditional importance of the group relative to all other groups. The right-hand plot shows the marginal importance of the group relative to the null model with geographic confounders only.

```{r dichot2groupvimp, fig.cap = "Group variable importance for predicting multiple sensitivity", fig.subcap = c("Conditional importance", "Marginal importance"), fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        dichotomous.2_grp_vimp_grob_lst <- dichotomous.2_vimp_plots
    } else {
        dichotomous.2_grp_vimp_grob_lst <- dichotomous.2_cv_vimp_plots
    }
    grid.arrange(grobs = list(dichotomous.2_grp_vimp_grob_lst$conditional, dichotomous.2_grp_vimp_grob_lst$marginal), ncol = 2)
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

We show the individual-level variable importance for the top `r num_pop_import` variables in Figure \@ref(fig:dichot2indivvimp). This importance is computed relative to the null model with geographic confounders only.

```{r dichot2indivvimp, fig.cap = "Individual variable importance for predicting multiple sensitivity", fig.height = 10, fig.width = 20}
## only report non-cv if no_cv
if (!reduce_outcomes) {
    if (no_cv) {
        dichotomous.2_grp_vimp_grob_lst <- dichotomous.2_vimp_plots
    } else {
        dichotomous.2_grp_vimp_grob_lst <- dichotomous.2_cv_vimp_plots
    }
    if (is.character(dichotomous.2_grp_vimp_grob_lst$individual)) {
        print(dichotomous.2_grp_vimp_grob_lst$individual)
    } else {
        grid.arrange(grobs = list(dichotomous.2_grp_vimp_grob_lst$individual), ncol = 1)
    }
} else {
    print("Only results for IC-50 are shown, since reduce_outcomes = TRUE.")
}
```

## Variable group definitions

Table \@ref(tab:vimpGroupTab) provides the individual HXB2 coordinates and variable names of the variables that make up each of the variable groups considered for importance.

``` {r vimpGroupTab}
## all_var_groups is created in preamble
var_grps_chr_lst <- lapply(lapply(all_var_groups, function(x) strsplit(x, ".", fixed = TRUE)), function(x) do.call(c, lapply(x, function(y) paste0(y[!grepl("hxb2", y) & !grepl("mer", y)], collapse = "."))))
grptab <- do.call(rbind, lapply(var_grps_chr_lst, function(x) paste0(x, collapse = ", ")))
kable(grptab, col.names = "Variables",
      digits = 3, row.names = TRUE,
      caption = "Individual variables within each variable group.")
```
